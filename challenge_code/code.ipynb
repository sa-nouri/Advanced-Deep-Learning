{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitadlvenvbd9a5c10f53f49eca40cc1477bddc462",
   "display_name": "Python 3.8.5 64-bit ('adl': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imbalance classfication problem\n",
    "\n",
    "The jupyter notebook includes all related algorithms and results for loading dataset, visualizing principal conponents, and, predicting the data's label using Logistic regression and a designed neural network. The problem will be examined several mehtods and ways.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Imbalanced classfication problem (Rare event prediction, Extreme event prediction, Severe class imbalance) refers to a class distribution that is inherently not balanced. As we will see, the classes distribution are not balanced.\n",
    " \n",
    "There are perhaps two main groups of causes for the imbalance we may want to consider; they are data sampling and properties of the domain. Errors may have been made when collecting the observations. One type of error might have been applying the wrong class labels to many examples. Alternately, the processes or systems from which examples were collected may have been damaged or impaired to cause the imbalance.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "NUM_PCA_COMPONENT: int = 2\n",
    "PCA_VARIANCE: np.float = 0.99\n",
    "NUM_LDA_COMPONENT: int = 2\n",
    "\n",
    "\n",
    "def get_data() -> List[pd.DataFrame]:\n",
    "    input_train_data = pd.read_csv('train_x.csv')\n",
    "    output_train_data = pd.read_csv('train_y.csv')\n",
    "    input_test_data = pd.read_csv('test_x.csv')\n",
    "    return input_train_data, output_train_data, input_test_data\n",
    "\n",
    "\n",
    "def standardize_data(input_train_data: pd.DataFrame, input_test_data: pd.DataFrame) -> List[np.ndarray]:\n",
    "    in_train = StandardScaler().fit_transform(input_train_data)\n",
    "    in_test = StandardScaler().fit_transform(input_test_data)\n",
    "    return in_train, in_test\n",
    "\n",
    "\n",
    "def project_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    pca = PCA(n_components=NUM_PCA_COMPONENT)\n",
    "    principalComponents = pca.fit_transform(data)\n",
    "    principalDf = pd.DataFrame(data = principalComponents,\n",
    "                                columns = ['principal component 1', 'principal component 2'])\n",
    "    return principalDf, pca\n",
    "\n",
    "\n",
    "def visualize_data_pca(final_df: pd.DataFrame) -> plt.plot:\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "    ax.set_title('2 component PCA', fontsize = 20)\n",
    "    targets = [1, 0]\n",
    "    colors = ['g', 'r']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = final_df['target'] == target\n",
    "        ax.scatter(final_df.loc[indicesToKeep, 'principal component 1'],\n",
    "                    final_df.loc[indicesToKeep, 'principal component 2'],\n",
    "                    c = color,\n",
    "                    s = 50)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "def visualize_data_lda(input_train_data: pd.DataFrame, out_train_data: pd.DataFrame) -> plt.plot:\n",
    "    y = out_train_data.target\n",
    "    X = input_train_data\n",
    "    y.loc[4001] = y.loc[2] + 2\n",
    "    X.loc[4001] = input_train_data.loc[0] * 10\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=NUM_LDA_COMPONENT)\n",
    "    # X_r = lda.fit(X, y).transform(X)\n",
    "    X_lda = lda.fit_transform(X, y)\n",
    "    colors = ['navy', 'red']\n",
    "    target_names = ['False', 'True']\n",
    "    plt.figure()\n",
    "    for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "        plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=.8, color=color,\n",
    "                    label=target_name)\n",
    "    plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    plt.title('2 LDA of data')\n",
    "    plt.xlabel('LDA 1')\n",
    "    plt.ylabel('LDA 2')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n",
    "    print(f\"The LDA variance ratios are {lda.explained_variance_ratio_}\\n\")"
   ]
  },
  {
   "source": [
    "# Examining the possibility of exisiting imbalance in the dataset\n",
    "\n",
    "In this step, the train data and test data will be loaded, then, we should examine the number of data in each classes to evaluate the possibility of existing imbalance between classes. Besides, the histogram of data will be plotted to examine the issue.\n",
    "\n",
    "### Two types of Imbalance:\n",
    "\n",
    "Slight Imbalance. An imbalanced classification problem where the distribution of examples is uneven by a small amount in the training dataset (e.g. 4:6).\n",
    "\n",
    "Severe Imbalance. An imbalanced classification problem where the distribution of examples is uneven by a large amount in the training dataset (e.g. 1:100 or more).\n",
    "\n",
    "#### Problem:\n",
    "\n",
    "The minority class is harder to predict because there are few examples of this class, by definition."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_data, out_train_data, input_test_data = get_data()\n",
    "data = input_train_data.copy()\n",
    "data['target'] = out_train_data['target']\n",
    "classes_ratio = (len(data[data['target'] == 1]) / len(data[data['target'] == 0]))\n",
    "if classes_ratio <= 0.6:\n",
    "    print(\"There exists severe imablance!!\\n\")\n",
    "if classes_ratio >= 0.6 and classes_ratio <= 0.95:\n",
    "    print(\"There exists slight imbalance!!\\n\")\n",
    "\n",
    "data.target.value_counts().plot(kind='bar', title='Count (target)', color = ['b', 'g'], grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data.groupby(\"target\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[0], group[1], marker=\"o\", linestyle=\"\", label=name)\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "As the figure above idicates, the frequency of classes is not balanced, and the first class (Target == zero) has much more samples than the other."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Visualize Principal Components\n",
    "\n",
    "The dataset should be normalized in order to avoid any problem not only in learning phase but also in computing principal components. Eventually, the data will be shown by considering principal components.\n",
    "\n",
    "Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales. The algorithm standardize approach is the transformation of the data onto unit scale (mean=0 and variance=1), which is a requirement for the optimal performance of many machine learning algorithms. Moreover, Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data.\n",
    "\n",
    "Here we plot the different samples on the 2 first principal components."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train, in_test = standardize_data(input_train_data, input_test_data)\n",
    "principal_df, pca = project_data(in_train)\n",
    "final_df = pd.concat([principal_df, out_train_data[['target']]], axis = 1)\n",
    "visualize_data_pca(final_df)\n",
    "print(f\"The pca's variance ratio is {pca.explained_variance_ratio_}\\n\")"
   ]
  },
  {
   "source": [
    "The plot of the dataset is created showing the large mass of examples for the majority class (red) and a small number of examples for the minority class (green), with some class overlap."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "Teh plot of the dataset is created showing the large mass of examples for the majority class (blue) and a small number of examples for the minority class (red), with some class overlap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data_lda(input_train_data, out_train_data)"
   ]
  },
  {
   "source": [
    "# Applying PCA to the Data\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. As defined in the first cell, the pcas' variance is 0.98, so there are forty five principal components for this value. By increasing its value, the number of principal components will increase and vice-versa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(PCA_VARIANCE)\n",
    "pca.fit(in_train)\n",
    "print(f\"Num of principal components is {pca.n_components_}\\n\")\n",
    "train_ = pca.transform(in_train)\n",
    "test_ = pca.transform(in_test)"
   ]
  },
  {
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The fitting problem for train data will be computed by using Logistic Regression in this step. Then, the fitting accuracy will be computed for the seperated test data. Eventually, the target for the test data will be calculated using the obtained parameters in LogisticRegression. And, finally, the result is saved as a CSV file.\n",
    "\n",
    "* accuracy (fraction of correct predictions): correct predictions / total number of data points"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, out_train_data.target, test_size=1/5.0, random_state=0)\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_1, train_lbl)\n",
    "\n",
    "print(f\"The prediction score on test:    {str(logisticRegr.score(test_1, test_lbl))}\\n\")\n",
    "print(f\"The prediction score on train:   {str(logisticRegr.score(train_1, train_lbl))}\\n\")\n",
    "\n",
    "predicted_test = logisticRegr.predict(test_)\n",
    "test_y = pd.DataFrame(predicted_test, columns=['target'])\n",
    "test_y['Unnamed: 0'] = out_train_data['Unnamed: 0']\n",
    "test_y = test_y[['Unnamed: 0', 'target']]\n",
    "test_y.to_csv('test_y.csv')\n",
    "test_y.head(8) "
   ]
  },
  {
   "source": [
    "## NOTES on Logisitic Regression:\n",
    "\n",
    "1) The difference betweeen train score and test score is not significant, then, there does not exist any overfitting in the logistic regression fitting.\n",
    "\n",
    "2) As the results of prediction clarify, all of the predicted targets are zero, which indicate that the logistic regression is not able to solve the imbalance classification problem, and causes the following problem. \n",
    "\n",
    "* The abundance of samples from the majority class can swamp the minority class. Most machine learning algorithms for classification predictive models are designed and demonstrated on problems that assume an equal distribution of classes. This means that a naive application of a model may focus on learning the characteristics of the abundant observations only, neglecting the examples from the minority class that is, in fact, of more interest and whose predictions are more valuable.\n",
    "\n",
    "* The learning process of most classification algorithms is often biased toward the majority class examples, so that minority ones are not well modeled into the final system.\n",
    "\n",
    "Hence, we are going to use deep learning methods to solve the problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Neural Network (Deep Learning)\n",
    "\n",
    "Deep learning uses an artificial neural network that uses multiple layers to progressively extract higher level features from the training data. We are using a simple three-layer network without any optimisation, except the usage of a small validation dataset. \n",
    "\n",
    "\n",
    "# Regularisation\n",
    "\n",
    "In the following, I applied the 3 best practices for handling overfitting in a neural network:\n",
    "\n",
    "1) reduce the networkâ€™s size\n",
    "\n",
    "2) adding some weight regularisation\n",
    "\n",
    "3) adding dropout\n",
    "\n",
    "## Metrics\n",
    "\n",
    "There are many metrics for evaluating how good a binary classifier is doing in predicting the class labels for instances/examples. Below are some caveats and suggestions for choosing and interpreting the appropriate metrics.\n",
    "\n",
    "* Accuracy can be misleading. Since accuracy is simple the ratio of correctly predicted instances over all instances used for evaluation, it is possible to get a decent accuracy while having mostly incorrect predictions for the minority class.\n",
    "\n",
    "* Confusion matrix helps break down the predictive performances on different classes.\n",
    "\n",
    "Therefore, we utilize a combination of confusion matrix metrics and specifity-sensivity for solving the problem. Beside, we can compute the classes' weight, then use these weights for fitting problem in order to decrease imbalancing.\n",
    "\n",
    "Split the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where overfitting is a significant concern from the lack of training data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, losses, metrics, regularizers, callbacks, initializers\n",
    "\n",
    "bias_initializer = initializers.HeNormal()\n",
    "\n",
    "NUM_VALIDATION: int = int(len(train_) * 0.2)\n",
    "\n",
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, out_train_data.target, test_size=1/8.0, random_state=0)\n",
    "x_validation = train_1[:NUM_VALIDATION]\n",
    "x_partial_train = train_1[NUM_VALIDATION:]\n",
    "y_validation = train_lbl[:NUM_VALIDATION]\n",
    "y_partial_train = train_lbl[NUM_VALIDATION:]\n",
    "\n",
    "# weight_for_0: np.float = 1.0 / len(out_train_data[out_train_data['target'] ==0])\n",
    "# weight_for_1: np.float = 1.0 / len(out_train_data[out_train_data['target'] ==1])\n",
    "weight_for_0 = (1 / len(out_train_data[out_train_data['target'] ==0])) * (len(out_train_data)) / 2.0\n",
    "weight_for_1 = (1 / len(out_train_data[out_train_data['target'] ==1])) * (len(out_train_data)) / 2.0\n",
    "class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "metrics_ = [\n",
    "    metrics.FalseNegatives(name=\"fn\"),\n",
    "    metrics.FalsePositives(name=\"fp\"),\n",
    "    metrics.TrueNegatives(name=\"tn\"),\n",
    "    metrics.TruePositives(name=\"tp\"),\n",
    "    metrics.Precision(name=\"precision\"),\n",
    "    metrics.Recall(name=\"recall\"),\n",
    "    metrics.AUC(name='auc'),\n",
    "    metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "\n",
    "def dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "def regularized_dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.6))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_prc', \n",
    "                                        verbose=1,\n",
    "                                        patience=10,\n",
    "                                        mode='max',\n",
    "                                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model = dl_model()\n",
    "model.fit(x_partial_train, y_partial_train,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        verbose=2,\n",
    "        shuffle=True,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        class_weight=class_weights)\n",
    "\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train: {str(model.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model_regularized = regularized_dl_model()\n",
    "model_regularized.fit(x_partial_train, y_partial_train,\n",
    "        epochs=300,\n",
    "        batch_size=64,\n",
    "        verbose=2,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        class_weight=class_weights)\n",
    "\n",
    "print(\"\\n\")\n",
    "model_regularized.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model_regularized.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train:  {str(model_regularized.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "source": [
    "## Notes on Neural Nework's result\n",
    "\n",
    "* As the test score of the first model is higher than the regularized model, we use this model to predict targets of the test dataset.\n",
    "\n",
    "* The regularized model has much less train/total parameter, then it's architecture has benefits rather than the firs one.\n",
    "\n",
    "* According to the result, using the mentioned metrics cause to overcome to imbalance problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Manipulation\n",
    "\n",
    "There exists several ways to balance the number of smaples in each classes. As mentioned before, there exists severe imbalance between the classes. As shown above, Most machine learning techniques will ignore the minority class, and thus, perform poorly on the observations we are most interested in. Typical challenges are biased predictions and appropriate selection of score type. There are multiple approaches to address data imbalance and the approaches could primarily focus on random over/undersampling and SMOTE (Synthetic Minority Oversampling Technique) for tabular data. I am going to explain and use SMOTE method for augmenting data.\n",
    "\n",
    "\n",
    "\n",
    "## Upsampling using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "A data augmentation technique for tabular data. SMOTE is available in Python using the imblearn library. SMOTE creates new data points based on the existing minority class data points using linear combinations of feature vectors. \n",
    "\n",
    "In the folllowing steps, the dataset will be augmented, then, the previous approaches/algorithms will be applied to examine the results.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "numbers_id = rng.choice(3 * len(data_train), size=(len(data_train) - len(input_train_data)),  replace=False)\n",
    "\n",
    "input_train_data, out_train_data, input_test_data = get_data()\n",
    "\n",
    "over = SMOTE(sampling_strategy=1)\n",
    "data_train, out = over.fit_resample(input_train_data, out_train_data['target'])\n",
    "\n",
    "data_target = pd.DataFrame(out, columns=['target'])\n",
    "data_train[\"Unnamed: 0\"].loc[len(input_train_data):] = numbers_id * data_train[\"Unnamed: 0\"].loc[len(input_train_data):]\n",
    "data_target['Unnamed: 0'] = data_train['Unnamed: 0']\n",
    "data_target = data_target[['Unnamed: 0', 'target']]\n",
    "print(len(data_train['Unnamed: 0'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_train.copy()\n",
    "data['target'] = data_target['target']\n",
    "classes_ratio = (len(data[data['target'] == 1]) / len(data[data['target'] == 0]))\n",
    "if classes_ratio <= 0.6:\n",
    "    print(\"There exists severe imablance!!\")\n",
    "if classes_ratio >= 0.6 and classes_ratio <= 0.95:\n",
    "    print(\"There exists slight imbalance!!\")\n",
    "else:\n",
    "    print(\"Dataset balancing looks good!!!\")\n",
    "\n",
    "data.target.value_counts().plot(kind='bar', title='Count (target)', color = ['b', 'g'], grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train, in_test = standardize_data(data_train, input_test_data)\n",
    "principal_df, pca = project_data(in_train)\n",
    "final_df = pd.concat([principal_df, data_target['target']], axis=1)\n",
    "visualize_data_pca(final_df)\n",
    "print(f\"The pca's variance ratio is {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data_lda(data_train, data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(PCA_VARIANCE)\n",
    "pca.fit(in_train)\n",
    "print(f\"Num of principal components is {pca.n_components_}\")\n",
    "train_ = pca.transform(in_train)\n",
    "test_ = pca.transform(in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, data_target.target, test_size=1/7.0, random_state=0)\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(train_1, train_lbl)\n",
    "print(\"The prediction score on test: \" + str(logisticRegr.score(test_1, test_lbl)))\n",
    "print(\"The prediction score on train: \"+ str(logisticRegr.score(train_1, train_lbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, losses, metrics, regularizers, initializers, callbacks\n",
    "\n",
    "\n",
    "NUM_VALIDATION: int = int(len(train_) * 0.2)\n",
    "\n",
    "bias_initializer = initializers.HeNormal()\n",
    "\n",
    "train_1, test_1, train_lbl, test_lbl = train_test_split(train_, data_target.target, test_size=1/5.0, random_state=0)\n",
    "x_validation = train_1[:NUM_VALIDATION]\n",
    "x_partial_train = train_1[NUM_VALIDATION:]\n",
    "y_validation = train_lbl[:NUM_VALIDATION]\n",
    "y_partial_train = train_lbl[NUM_VALIDATION:]\n",
    "\n",
    "metrics_ = [\n",
    "    metrics.FalseNegatives(name=\"fn\"),\n",
    "    metrics.FalsePositives(name=\"fp\"),\n",
    "    metrics.TrueNegatives(name=\"tn\"),\n",
    "    metrics.TruePositives(name=\"tp\"),\n",
    "    metrics.Precision(name=\"precision\"),\n",
    "    metrics.Recall(name=\"recall\"),\n",
    "    metrics.AUC(name='auc'),\n",
    "    metrics.AUC(name='prc', curve='PR')\n",
    "]\n",
    "\n",
    "\n",
    "def dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    # model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', bias_initializer=bias_initializer))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "def regularized_dl_model() -> models.Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation=\"relu\", input_shape=(train_1.shape[-1],)))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-4),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=metrics_)\n",
    "    return model\n",
    "\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_prc', \n",
    "                                        verbose=1,\n",
    "                                        patience=10,\n",
    "                                        mode='max',\n",
    "                                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = dl_model()\n",
    "model.fit(x_partial_train, y_partial_train,\n",
    "        epochs=1500,\n",
    "        batch_size=128,\n",
    "        verbose=2,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping])\n",
    "\n",
    "print(\"\\n\")\n",
    "model.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train: {str(model.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model_regularized = regularized_dl_model()\n",
    "model_regularized_history = model_regularized.fit(x_partial_train, y_partial_train,\n",
    "        epochs=1000,\n",
    "        batch_size=64,\n",
    "        verbose=2,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        shuffle=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "model_regularized.summary()\n",
    "print(\"\\n\")\n",
    "print(f\"score on test:  {str(model_regularized.evaluate(test_1, test_lbl)[1])}\\n\")\n",
    "print(f\"score on train:  {str(model_regularized.evaluate(train_1, train_lbl)[1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history: callbacks.History) -> plt.plot:\n",
    "  # Use a log scale on y-axis to show the wide range of values.\n",
    "  plt.semilogy(history.epoch, history.history['loss'],\n",
    "               color='b', label='Train')\n",
    "  plt.semilogy(history.epoch, history.history['val_loss'],\n",
    "               color='r', label='Val',\n",
    "               linestyle=\"--\")\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  plt.title(\"Trai and validation loss\")\n",
    "  plt.ylabel('Loss')\n",
    "\n",
    "plot_loss(model_regularized_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'prc', 'precision', 'recall']\n",
    "  colors = ['b', 'g', 'r', 'y']\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color='b', label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color='r', linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "plot_metrics(model_regularized_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions\n",
    "train_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "\n",
    "predictions = model_regularized.predict(train_1)\n",
    "plot_cm(train_lbl, predictions)"
   ]
  },
  {
   "source": [
    "If the model had predicted everything perfectly, this would be a diagonal matrix where values off the main diagonal, indicating incorrect predictions, would be zero. In this case the matrix shows that you have relatively few false positives, meaning that there were relatively few legitimate transactions that were incorrectly flagged. However, you would likely want to have even fewer false negatives despite the cost of increasing the number of false positives. This trade off may be preferable because false negatives would allow fraudulent transactions to go through, whereas false positives may cause an email to be sent to a customer to ask them to verify their card activity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = roc_curve(labels, predictions)\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.5,20])\n",
    "  plt.ylim([80,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "train_predictions_baseline = model_regularized.predict(train_1)\n",
    "test_predictions_baseline = model_regularized.predict(test_1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_roc(\"Train Baseline\", train_lbl, train_predictions_baseline, color='b')\n",
    "plot_roc(\"Test Baseline\", test_lbl, test_predictions_baseline, color='r', linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "source": [
    "Teh ROC plot, which is depicted above,is useful because it shows, at a glance, the range of performance the model can reach just by tuning the output threshold.\n",
    "\n",
    "\n",
    "### AUPRC\n",
    "\n",
    "Now plot the AUPRC. Area under the interpolated precision-recall curve, obtained by plotting (recall, precision) points for different values of the classification threshold. Depending on how it's calculated, PR AUC may be equivalent to the average precision of the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_prc(name, labels, predictions, **kwargs):\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "\n",
    "    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "train_predictions_baseline = model_regularized.predict(train_1)\n",
    "test_predictions_baseline = model_regularized.predict(test_1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_prc(\"Train Baseline\", train_lbl, train_predictions_baseline, color='b')\n",
    "plot_prc(\"Test Baseline\", test_lbl, test_predictions_baseline, color='r', linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model.predict_classes(test_)\n",
    "predictions = model_regularized.predict_classes(test_)\n",
    "test_y = pd.DataFrame(predictions, columns=['target'])\n",
    "test_y['Unnamed: 0'] = out_train_data['Unnamed: 0']\n",
    "test_y = test_y[['Unnamed: 0', 'target']]\n",
    "test_y.to_csv('test_y.csv')\n",
    "test_y.head(8) \n",
    "sum(test_y['target'])"
   ]
  }
 ]
}